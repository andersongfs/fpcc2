---
title: "lab3cp3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# manipulate data
require(dplyr, warn.conflicts = FALSE)
require(reshape2) # transform dataframe to wide format
require(tidyr)
# read data
require(readr)
# plot
require(ggplot2)
require(plotly)
# plot correlations
require(corrplot)
```
## Leitura dos dados
  Nesta atividade utilizaremos um _dataset_ com informações sobre as diferentes pós graduações brasileiras na área de informática e tecnologia. Encontramos as seguintes variáveis disponíveis para a análise:
```{r}
# leitura e filtragem dos dados
ppg = read_csv(file = "../data/capes-cacc.csv")
names(ppg)
```
  A partir dessas variáveis, vamos nos ater a estas:
  1. Instituição: nome da instituição de ensino 
  2. Sigla: sigla da instituição de ensino
  3. Nível: nível do curso de pós graduação da instituição de ensino perante a CAPES
  4. qtd_artigo_por_prof: quantidade de artigos publicados em conferência, dividido pela soma de Docentes colaboradores com Docentes permanentes
  5. trabalhos_de_conclusao: soma do número de Dissertações com o número de Teses
  6. periodicos_excelencia: total de periódicos publicados com qualis A1 ou A2 ou B1
  7. periodicos_outros: total de periódicos publicados com qualis diferente dos de excelência

```{r}

# substitui NA por 0
ppg[is.na(ppg)] = 0

# gerando novas variaveis a partir das existentes
ppg_plus = ppg %>% 
  mutate(periodicos_excelencia = periodicos_A1 + periodicos_A2 + periodicos_B1) %>% 
  mutate(periodicos_outros = periodicos_B2 + periodicos_B3 + periodicos_B4 + periodicos_B5 + periodicos_C + periodicos_NA) %>% 
  mutate(qtd_artigo_por_prof = `Artigos em conf` / ( `Docentes colaboradores` + `Docentes permanentes` )) %>% 
  mutate(trabalhos_de_conclusao = Dissertacoes + Teses)

ppg_filtered = ppg_plus %>% 
  select(`Instituição`, `Sigla`, `Nível`, qtd_artigo_por_prof, trabalhos_de_conclusao, periodicos_excelencia, periodicos_outros)

```


# Visão geral dos dados
  Para termos uma visão geral dos dados, vamos observar o histograma dos valores.
```{r}
# data overview
# summary(ppg_filtered)
# histogram
ppg_filtered %>%
    gather(key = "variavel", value = "valor", -`Instituição`, -`Sigla`) %>%
    ggplot(aes(x = valor)) +
    geom_histogram(fill = "white", color = "black", bins = 20) +
    facet_grid(. ~ variavel, scales = "free_x")

```
  Com o histograma podemos ver principalmente que grande parte dos programas de pós-graduação possuem nota 3 perante a CAPES, poucos tem mais que 100 periódicos classificados como excelentes, aparentemente um programa tem um número bem elevado quando comparado com os outros de periódicos não excelentes e muitos tem menos que 5 trabalhos de conclusão.
  Para obtermos mais detalhes vamos fazer uso de boxplot para estas mesmas variáveis. Vejamos logo abaixo:
```{r warning=FALSE}
# boxplot
boxplot_all = ppg_filtered %>%
    gather(key = "variavel", value = "valor", -`Instituição`, -`Sigla`) %>%
    ggplot(aes(x = "", y = valor)) +
    geom_boxplot() +
    facet_grid(. ~ variavel, scales = "free_x")
ggplotly(boxplot_all)

```
  O boxplot nos acrescenta informações como, onde a maior quantidade de valores se concentra, e nos dá uma melhor visão dos pontos extremos, como em trabalhos de conclusão temos um programa com 577 trabalhos de conclusão(UNIVERSIDADE FEDERAL DE PERNAMBUCO), quando 75% dos programas tem menos que 115. Em periódicos de excelência temos apenas 25% das instituições tem muitos(65 a 355) trabalhos nesta categoria, enquanto 75% dos programas possuem no máximo 70 publicações nesta categoria.
  Abaixo veremos como é a correlação entre essas variáveis.

```{r}
# correlations 
matrix_cor = cor(ppg_filtered %>% select(-`Instituição`, -Sigla))
corrplot(matrix_cor, method = "number", type = "lower")
```

  Temos que a maioria da relações entre pares de variáveis possuem bastante correlação.

## Agrupamento
  Vamos agora utilizar um método de agruapamento para ver se podemos separar os dados em diferentes grupos. Antes disso, devemos normalizar os dados e buscar identificar a quantidade de clusters em que devemos dividir nossos dados .
```{r}
# scale dos dados
ppg_scaled = ppg_filtered %>% 
  mutate_at(vars(`Nível`:periodicos_outros), funs(as.numeric(scale(.))))
```

```{r}
# Determine number of clusters
wss <- (nrow(ppg_scaled[3:7])-1)*sum(apply(ppg_scaled[3:7],2,var))
for (i in 2:15) wss[i] <- sum(kmeans(ppg_scaled[3:7], 
  	centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")

```
  Com o gráfico acima notamos que o erro acentua bastante a quando passamos de 3 para 2 clusters e pouco muda de 4 para 3. Então vamos usar o k-means para fazer o agrupamento passando como número de clusters requeridos, 3.
```{r}
# K-Means Cluster Analysis
fit <- kmeans(ppg_scaled[3:7], centers = 3, nstart = 20) # 3 cluster solution

# get cluster means 
aggregate(ppg_scaled[3:7],by=list(fit$cluster),FUN=mean)

# append cluster assignment
ppg_scaled <- data.frame(ppg_scaled[3:7], fit$cluster)
```


```{r}
# plot parallel coordinates
ppg_scaled$.row <- rownames(ppg_scaled)
ppg_wide <- melt(ppg_scaled, id = c(".row", "fit.cluster") )

parallel_plot = ggplot(ppg_wide, aes(x = variable, y = value, group = .row, colour = fit.cluster)) + 
    theme(axis.text.x = element_text(angle = 45, hjust = 0)) +
    geom_line(alpha = .2) + 
    facet_wrap(~ fit.cluster)
ggplotly(parallel_plot)
```
  Este é o resultado que obtivemos. Três grupos os quais podemos dizer que o primeiro são os programas de pós medianos, ficam perto do 0, que é o media dos valores, e possuem valores para as outras variáveis também na mesma faixa, com a maioria deles entre 0 e 1. O grupo dois podemos nomeá-lo como sendo o grupo de menor _status_ com valores em geral abaixo da média. Por fim o terceiro, o grupo com maior quantidade de periódicos excelentes e trabalhos de conclusão principalmente. Mas uma curiosidade é a baixa taxa de artigos por professor, o que pode ser uma análise futura a busca por uma resposta para esse fato.
  
## Redução de dimensionalidade
  Nesta seção iremos reduzir a dimensionalidade para facilitar a visualização. Iremos sair de cinco dimensões para duas, utilizando a técnica _principal component analysis - PCA_.
  Abaixo temos uma tabela a qual mostra a "equação" que representa cada dimensionalidade tendo as variáveis como fatores e um gráfico indicando a variância para diferentes quantidades de dimensões. 
```{r}
ppg.pca <- prcomp(ppg_scaled[1:5],center = TRUE) 
print(ppg.pca)
plot(ppg.pca, type = "l")

```
  A variância diminui muito a partir de duas dimensões mas não difere muito entre 2 e 3 dimensões, o que indica algo bom. Duas dimensões já explicam bastante dos dados.
```{r warning=FALSE, message=FALSE}
library(devtools)
#install_github("ggbiplot", "vqv")
 
library(ggbiplot)
ppg.groups = ppg_scaled[, 6] #fit.cluster column
pca_plot <- ggbiplot(ppg.pca, obs.scale = 1, var.scale = 1, 
              groups = ppg.groups, ellipse = TRUE, 
              circle = FALSE) + 
  scale_color_continuous(name = '') + 
  theme(legend.direction = 'horizontal', 
               legend.position = 'top')
print(pca_plot)
```
  Por fim, temos nosso conjuntos de dados reduzido a duas dimensões e cada cor é referente a um dos grupos que foram formados anteriormente. 
  Observamos que o Nível está intimamente relacionado com as variáveis que contam os periódicos e trabalhos de conclusão, estando eles quase alinhado com o eixo x. Examentente pela maioria das variáveis estarem "alinhadas" com o eixo x(PC 1), temos que só esse eixo já explica 66.5% dos nossos dados e a outra dimensão, PC 2, explica apenas 17.4%. Além disso, podemos dizer que a variável **quantidade de artigos por professor** não influencia muito a variável nível.
  
```{r include=FALSE}
# Ploting k-means groups
# vary parameters for most readable graph
#library(cluster) 
#clusplot(ppg_scaled, fit$cluster, color=TRUE, shade=TRUE, 
#  	labels=2, lines=0)

# Centroid Plot against 1st 2 discriminant functions
#library(fpc)
#plotcluster(ppg_scaled, fit$cluster)

```
